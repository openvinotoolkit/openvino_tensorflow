{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Colab_OpenVINO_TensorFlow_classification_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT0u-5Ix-JG5"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_tensorflow/blob/master/docs/cloud_instructions/Colab_OpenVINO_TensorFlow_classification_example.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0NqXHZ9pg82"
      },
      "source": [
        "## INSTALL OpenVINO™ integration with TensorFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOk7Kisgh1Nw",
        "outputId": "2fcaf390-2ffc-46c3-e485-7d1547239b82"
      },
      "source": [
        "# Upload the required wheel files, models and images in a google drive folder\n",
        "# Uncomment and run the below command to copy them in your current workspace\n",
        "#!cp /content/drive/MyDrive/TF-OV/working_dir_files/* . \n",
        "\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install pillow\n",
        "\n",
        "# Install stock TensorFlow\n",
        "!pip -q install tensorflow==2.6.0 \n",
        "\n",
        "# Install OpenVINO™ integration with TensorFlow\n",
        "!pip -q install openvino-tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "     |████████████████████████████████| 454.4 MB 9.4 kB/s             \n",
            "     |████████████████████████████████| 4.0 MB 42.8 MB/s            \n",
            "     |████████████████████████████████| 1.2 MB 77.8 MB/s            \n",
            "     |████████████████████████████████| 462 kB 58.7 MB/s            \n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "     |████████████████████████████████| 26.5 MB 19 kB/s             \n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgy6S1B_rW9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f53db5-820d-45b6-9221-ffca31019f3b"
      },
      "source": [
        "!ldd --version\n",
        "!python3 --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ldd (Ubuntu GLIBC 2.27-3ubuntu1.3) 2.27\n",
            "Copyright (C) 2018 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "Written by Roland McGrath and Ulrich Drepper.\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MkE439_ybZB",
        "outputId": "e81af597-e823-4868-f1d2-c47156066aff"
      },
      "source": [
        "!git clone --quiet https://github.com/openvinotoolkit/openvino_tensorflow.git\n",
        "%cd openvino_tensorflow\n",
        "!git submodule init\n",
        "!git submodule update --recursive\n",
        "%cd ..\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/openvino_tensorflow\n",
            "Submodule 'ocm' (https://github.com/intel/ocm) registered for path 'ocm'\n",
            "Cloning into '/content/openvino_tensorflow/ocm'...\n",
            "Submodule path 'ocm': checked out '1536f9877b8817872aa0e96859ac6547c1850c96'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D2yhLwy09P9"
      },
      "source": [
        "# Lets get the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXgPfp4HxtG2",
        "outputId": "41817cda-dac5-445b-fd60-483130a873fb"
      },
      "source": [
        "!curl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" | tar -C openvino_tensorflow/examples/data -xz \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 84.5M  100 84.5M    0     0  49.9M      0  0:00:01  0:00:01 --:--:-- 49.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDQDLFx69izd"
      },
      "source": [
        "# Now lets infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EImyzDiiHGW"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from tensorflow.keras import backend as K\n",
        "from IPython.display import HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import openvino_tensorflow as ovtf\n",
        "from subprocess import check_output, call\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzNof2kniUfj"
      },
      "source": [
        "def load_graph(model_file):\n",
        "    \"\"\"Parse and load model from the protobuf binary(pb) file\n",
        "\n",
        "    Parameters:\n",
        "    model_file (string): Path of the model pb file\n",
        "\n",
        "    Returns:\n",
        "    graph: TF graph object for the parsed model\n",
        "    \"\"\"  \n",
        "    graph = tf.Graph()\n",
        "    graph_def = tf.compat.v1.GraphDef()\n",
        "\n",
        "    assert os.path.exists(model_file), \"Could not find directory\"\n",
        "    with open(model_file, \"rb\") as f:\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    with graph.as_default():\n",
        "        tf.import_graph_def(graph_def)\n",
        "\n",
        "    return graph"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Hlw2oDZXgu"
      },
      "source": [
        "\n",
        "def read_tensor_from_image_file(file_name,\n",
        "                                input_height=299,\n",
        "                                input_width=299,\n",
        "                                input_mean=0,\n",
        "                                input_std=255):\n",
        "    \"\"\"Reads input image file, resize it to given input height and width.\n",
        "       Normalize the image using input mean and standard deviation and returns the image tensor\n",
        "    \"\"\"  \n",
        "    input_name = \"file_reader\"\n",
        "    output_name = \"normalized\"\n",
        "    file_reader = tf.io.read_file(file_name, input_name)\n",
        "    if file_name.endswith(\".png\"):\n",
        "        image_reader = tf.image.decode_png(\n",
        "            file_reader, channels=3, name=\"png_reader\")\n",
        "    elif file_name.endswith(\".gif\"):\n",
        "        image_reader = tf.squeeze(\n",
        "            tf.image.decode_gif(file_reader, name=\"gif_reader\"))\n",
        "    elif file_name.endswith(\".bmp\"):\n",
        "        image_reader = tf.image.decode_bmp(file_reader, name=\"bmp_reader\")\n",
        "    else:\n",
        "        image_reader = tf.image.decode_jpeg(\n",
        "            file_reader, channels=3, name=\"jpeg_reader\")\n",
        "    float_caster = tf.cast(image_reader, tf.float32)\n",
        "    dims_expander = tf.expand_dims(float_caster, 0)\n",
        "    resized = tf.compat.v1.image.resize_bilinear(dims_expander,\n",
        "                                                 [input_height, input_width])\n",
        "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
        "    sess = tf.compat.v1.Session()\n",
        "    result = sess.run(normalized)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt54fcWXZbZA"
      },
      "source": [
        "def load_labels(label_file):\n",
        "    \"\"\"Parses the label file, assuming that labels are separated with a newline\n",
        "       in the file and returns the list of labels.\n",
        "    \"\"\"  \n",
        "    label = []\n",
        "    proto_as_ascii_lines = tf.io.gfile.GFile(label_file).readlines()\n",
        "    for l in proto_as_ascii_lines:\n",
        "        label.append(l.rstrip())\n",
        "    return label"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e812DUX92T43"
      },
      "source": [
        "def infer_openvino_tensorflow(model_file, input_layer, output_layer, file_name , input_height, input_width, input_mean, input_std, label_file):\n",
        "    \"\"\"Takes the tensorflow model and all other input parameters as arguments. \n",
        "       Run inference on the classification model and prints the predictions.\n",
        "    \"\"\"\n",
        "    print(\"CREATE MODEL - BEGIN\")\n",
        "    graph = load_graph(model_file)\n",
        "    print(\"CREATE MODEL - END\")\n",
        "\n",
        "    input_name = \"import/\" + input_layer\n",
        "    output_name = \"import/\" + output_layer\n",
        "    input_operation = graph.get_operation_by_name(input_name)\n",
        "    output_operation = graph.get_operation_by_name(output_name)\n",
        "\n",
        "\n",
        "    # update config params for openvino tensorflow\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config_ngraph_enabled = ovtf.update_config(config)\n",
        "\n",
        "    print(\"PREDICTION - BEGIN\") \n",
        "\n",
        "    with tf.compat.v1.Session(\n",
        "            graph=graph, config=config_ngraph_enabled) as sess:\n",
        "        t = read_tensor_from_image_file(\n",
        "            file_name,\n",
        "            input_height=input_height,\n",
        "            input_width=input_width,\n",
        "            input_mean=input_mean,\n",
        "            input_std=input_std)\n",
        "        # Warmup\n",
        "        results = sess.run(output_operation.outputs[0],\n",
        "                           {input_operation.outputs[0]: t})\n",
        "        # Run\n",
        "        \n",
        "        for num_times in range(10):\n",
        "            start = time.time()\n",
        "            results = sess.run(output_operation.outputs[0],\n",
        "                               {input_operation.outputs[0]: t})\n",
        "            elapsed = time.time() - start\n",
        "            print('Inference time in ms: %f' % (elapsed * 1000))\n",
        "            \n",
        "    print(\"PREDICTION - END\")\n",
        "    results = np.squeeze(results)\n",
        "\n",
        "    if label_file:\n",
        "        top_k = results.argsort()[-5:][::-1]\n",
        "        labels = load_labels(label_file)\n",
        "        for i in top_k:\n",
        "            print(labels[i], results[i])\n",
        "    else:\n",
        "        print(\"No label file provided. Cannot print classification results\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wxwq5184tkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80322960-d2ad-45f2-8c92-438abf2b1149"
      },
      "source": [
        "%cd openvino_tensorflow/examples/data/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/openvino_tensorflow/examples/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dr7HWdgZds6",
        "outputId": "484dfb3d-9f85-430e-d98f-a72ac7d057c0"
      },
      "source": [
        "file_name = \"grace_hopper.jpg\"\n",
        "model_file = \"inception_v3_2016_08_28_frozen.pb\"\n",
        "label_file = \"imagenet_slim_labels.txt\"\n",
        "input_height = 299\n",
        "input_width = 299\n",
        "input_mean = 0\n",
        "input_std = 255\n",
        "input_layer = \"input\"\n",
        "output_layer = \"InceptionV3/Predictions/Reshape_1\"\n",
        "backend_name = \"CPU\"\n",
        "    \n",
        "\n",
        "#Print list of available backends\n",
        "print('Available Backends:')\n",
        "backends_list = ovtf.list_backends()\n",
        "for backend in backends_list:\n",
        "    print(backend)\n",
        "ovtf.set_backend(backend_name)\n",
        "\n",
        "print(\"OpenVINO TensorFlow is enabled\")\n",
        "infer_openvino_tensorflow(model_file, input_layer, output_layer, file_name, input_height, input_width, input_mean, input_std, label_file )\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Backends:\n",
            "CPU\n",
            "OpenVINO TensorFlow is enabled\n",
            "CREATE MODEL - BEGIN\n",
            "CREATE MODEL - END\n",
            "PREDICTION - BEGIN\n",
            "Inference time in ms: 180.995464\n",
            "Inference time in ms: 181.902409\n",
            "Inference time in ms: 197.049379\n",
            "Inference time in ms: 193.738699\n",
            "Inference time in ms: 191.765070\n",
            "Inference time in ms: 191.222191\n",
            "Inference time in ms: 185.688019\n",
            "Inference time in ms: 191.789865\n",
            "Inference time in ms: 199.031115\n",
            "Inference time in ms: 186.476707\n",
            "PREDICTION - END\n",
            "military uniform 0.8343049\n",
            "mortarboard 0.021869816\n",
            "academic gown 0.010358169\n",
            "pickelhaube 0.008008191\n",
            "bulletproof vest 0.0053508924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQXt492OASb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a0abae-735b-4420-cd50-f684a3801d08"
      },
      "source": [
        "#Disable\n",
        "ovtf.disable()\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"OpenVINO TensorFlow is disabled\")\n",
        "infer_openvino_tensorflow(model_file, input_layer, output_layer, file_name, input_height, input_width, input_mean, input_std, label_file )\n",
        "ovtf.enable()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "OpenVINO TensorFlow is disabled\n",
            "CREATE MODEL - BEGIN\n",
            "CREATE MODEL - END\n",
            "PREDICTION - BEGIN\n",
            "Inference time in ms: 257.229567\n",
            "Inference time in ms: 239.981890\n",
            "Inference time in ms: 238.631248\n",
            "Inference time in ms: 242.352962\n",
            "Inference time in ms: 263.074160\n",
            "Inference time in ms: 241.921186\n",
            "Inference time in ms: 238.617659\n",
            "Inference time in ms: 245.656729\n",
            "Inference time in ms: 248.639584\n",
            "Inference time in ms: 238.406658\n",
            "PREDICTION - END\n",
            "military uniform 0.8343058\n",
            "mortarboard 0.02186954\n",
            "academic gown 0.010358081\n",
            "pickelhaube 0.0080081895\n",
            "bulletproof vest 0.0053509064\n"
          ]
        }
      ]
    }
  ]
}